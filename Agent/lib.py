# This file simply exists because langchain cant do shit
from logging_ import Logger
from ollama import chat
import json
from enum import Enum
from pydantic import BaseModel

class Tool:
    def __init__(self, name: str, func, description: str, code_name: str):
        self.name = name
        self.description = description
        self.func = func
        self.code_name = code_name
        self.logger = Logger("tool")
        self.logger.info(f"created tool {self.code_name}")
    def run(self):
        self.logger.info(f"running tool {self.code_name}")
        return self.func()

class Agent:
    def __init__(self, code_name: str, model: str, exec_tools: list[Tool], bs_model: type):
        self.code_name = code_name
        self.model = model
        self.exec_tools = exec_tools
        self.bs_model = bs_model
        self.logger = Logger("agent")
        self.logger.info(f"created agent \"{self.code_name}\", model: {self.model}")
    def pick(self, user_prompt):
        prompt = f"""
        You are a Model used to pick the right tool for prompts generated by the usesr
        prompt: {user_prompt}
        """
        response = chat(model=self.model, format=self.bs_model.model_json_schema(), messages=[{
            "role": "user",
            "content": prompt
        }])
        result = self.bs_model.model_validate_json(response['message']['content']).result.value
        return result
    def run(self, user_prompt):
        tool_name = self.pick(user_prompt)
        curr_tool = None
        for tool in self.exec_tools:
            if tool.name == tool_name:
                curr_tool = tool
                self.logger.info(f"[{self.model} OUTPUT] {curr_tool.code_name}")
        if not curr_tool:
            raise Exception("OUTPUT ERROR: output by the model does not satisfy the requirements of the given tools")
        return curr_tool.run()

def call_model(model_name: str, msg: str):
    return chat(model=model_name, messages=[{"role": "user", "content": msg}])['message']['content']
